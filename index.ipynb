{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ace4bcb150d5b97d72feacda06091be8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "Today you'll be creating several different linear regression models in a predictive machine learning context.\n",
    "\n",
    "In the cells below, we are importing relevant modules that you might need later on. We also load and prepare the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "fc810633c7dca8c292b918fe72fa5fdd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "64344faaa2aff0fd709b6cb7b30ab89c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.042500</td>\n",
       "      <td>23.264000</td>\n",
       "      <td>30.554000</td>\n",
       "      <td>14.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>85.854236</td>\n",
       "      <td>14.846809</td>\n",
       "      <td>21.778621</td>\n",
       "      <td>5.217457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.375000</td>\n",
       "      <td>9.975000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>218.825000</td>\n",
       "      <td>36.525000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>17.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296.400000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TV       radio   newspaper       sales\n",
       "count  200.000000  200.000000  200.000000  200.000000\n",
       "mean   147.042500   23.264000   30.554000   14.022500\n",
       "std     85.854236   14.846809   21.778621    5.217457\n",
       "min      0.700000    0.000000    0.300000    1.600000\n",
       "25%     74.375000    9.975000   12.750000   10.375000\n",
       "50%    149.750000   22.900000   25.750000   12.900000\n",
       "75%    218.825000   36.525000   45.100000   17.400000\n",
       "max    296.400000   49.600000  114.000000   27.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "data = pd.read_csv('raw_data/advertising.csv', index_col=0)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ff89f4de7bd81650243d398dabc6a494",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "X = data.drop('sales', axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "63f43bbf30615821bf67123b1e18b075",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y,random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1fb593eab3bc756b534afc39bcf3bcb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To make things simpler for the following examples, we will train models on `X_train` and `y_train`, and evaluate them on `X_test` and `y_test`, without the creation of any additional holdout validation set.\n",
    "\n",
    "### 1. We'd like to create linear regression models with a bit of added complexity, and we will do it by adding some polynomial terms. Write a function to calculate train and test error for different polynomial degrees.\n",
    "\n",
    "This function should:\n",
    "* take `degree` as a parameter that will be used to create polynomial features to be used in a linear regression model\n",
    "* create a PolynomialFeatures object for that degree and fit a linear regression model using the transformed data\n",
    "* calculate the mean square error for each level of polynomial\n",
    "* return the `train_error` and `test_error`\n",
    "\n",
    "Hint: use [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) from SciKit-Learn to preprocess the `X` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "f35fcc3bbd26df30e0882818d7ace548",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+ ------- + ----------- + ---------- +\n",
      "| Degrees | Train Error | Test Error |\n",
      "+ ------- + ----------- + ---------- +\n",
      "|    1    |      2.6669 |     3.3856 |\n",
      "|    2    |      0.4283 |     0.1864 |\n",
      "|    3    |      0.2424 |     0.1528 |\n",
      "|    4    |      0.1818 |     1.9523 |\n",
      "|    5    |      0.0651 |     5.6725 |\n",
      "|    6    |      0.0459 |    22.4071 |\n",
      "|    7    |      0.0313 |    24.1634 |\n",
      "|    8    |      0.0391 |   718.2957 |\n",
      "+ ------- + ----------- + ---------- +\n"
     ]
    }
   ],
   "source": [
    "def polynomial_regression(degree, X_train , X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Calculate train and test error for a linear regression with polynomial features.\n",
    "    \n",
    "    input: Polynomial degree, X for training, X for testing, y for training,\n",
    "    y for testing\n",
    "    output: A two-tuple containing the mean squared error for train and test set\n",
    "    \"\"\"\n",
    "    # 1. Instantiate a PolynomialFeatures object and create transformed\n",
    "    # versions of X_train and X_test\n",
    "    \n",
    "    # 2. Instantiate a linear regression and fit it to the transformed data\n",
    "    \n",
    "    # 3. Calculate the train and test MSEs, and return them as a tuple\n",
    "    \n",
    "    None\n",
    "### BEGIN SOLUTION\n",
    "    poly = PolynomialFeatures(degree=degree,interaction_only=False)\n",
    "    X_poly_train = poly.fit_transform(X_train)\n",
    "    X_poly_test = poly.transform(X_test)\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(X_poly_train,y_train)\n",
    "    train_error = mean_squared_error(y_train, lr_poly.predict(X_poly_train))\n",
    "    test_error = mean_squared_error(y_test, lr_poly.predict(X_poly_test))\n",
    "    return train_error, test_error\n",
    "\n",
    "from test_scripts.test_class import Test\n",
    "test = Test()\n",
    "\n",
    "degree_3 = polynomial_regression(3, X_train, X_test, y_train, y_test)\n",
    "degree_4 = polynomial_regression(4, X_train, X_test, y_train, y_test)\n",
    "\n",
    "test.save(degree_3, \"degree_3\")\n",
    "test.save(degree_4, \"degree_4\")\n",
    "### END SOLUTION\n",
    "\n",
    "# Use this code to test your solution\n",
    "print(\"\"\"\n",
    "+ ------- + ----------- + ---------- +\n",
    "| Degrees | Train Error | Test Error |\n",
    "+ ------- + ----------- + ---------- +\"\"\")\n",
    "for degree in range(1, 9):\n",
    "    train_error, test_error = polynomial_regression(degree, X_train, X_test, y_train, y_test)\n",
    "    print(\"| {:^7} | {:11.4f} | {:10.4f} |\".format(degree, train_error, test_error))\n",
    "print(\"+ ------- + ----------- + ---------- +\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ba146facebf026d7c3a422923a689605",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS\n",
    "\n",
    "degree_3 = polynomial_regression(3, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# The result of this function call should be a tuple containing two\n",
    "# floating point numbers\n",
    "assert type(degree_3) == tuple\n",
    "assert len(degree_3) == 2\n",
    "assert type(degree_3[0]) == np.float64 or type(degree_3[0]) == float\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "from test_scripts.test_class import Test\n",
    "test = Test()\n",
    "\n",
    "degree_4 = polynomial_regression(4, X_train, X_test, y_train, y_test)\n",
    "\n",
    "test.run_test(degree_3, \"degree_3\")\n",
    "test.run_test(degree_4, \"degree_4\")\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "327b1f4cd71d6a71b448ffcfa42c9a4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src =\"visuals/rsme_poly_2.png\" width = \"600\">\n",
    "\n",
    "<!---\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "degree = list(range(1, 10 + 1))\n",
    "ax.plot(degree, error_train[0:len(degree)], \"-\", label=\"Train Error\")\n",
    "ax.plot(degree, error_test[0:len(degree)], \"-\", label=\"Test Error\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Polynomial Feature Degree\")\n",
    "ax.set_ylabel(\"Root Mean Squared Error\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Relationship Between Degree and Error\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"visuals/rsme_poly.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->\n",
    "\n",
    "### 2. Refer to the plot of train and test errors for an example model above. What is the optimal number of degrees for our polynomial features in this model?\n",
    "\n",
    "### In general, how does increasing the polynomial degree relate to the Bias/Variance tradeoff?  (Note that this graph shows RMSE and not MSE.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "5d7316b28b2d55e0ce30467e199a15df",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "The optimal number of features in this example is 3 because the testing error is minimized at this point, and it increases dramatically with a higher degree polynomial.\n",
    "\n",
    "As we increase the polynomial features, it is going to cause our training error to decrease, which decreases the bias but increases the variance (the testing error increases).\n",
    "\n",
    "In other words, the more complex the model, the higher the chance of overfitting.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "adda862983eb04242e2163980e970230",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3. In general what methods would you can use to reduce overfitting and underfitting? Provide an example for both and explain how each technique works to reduce the problems of underfitting and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "e4ad4a8f65419763152b3263216cea4d",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "Overfitting: Regularization. With regularization, more complex models are penalized. This ensures that the models are not trained to too much \"noise.\"\n",
    "\n",
    "Underfitting: Feature engineering. By adding additional features, you enable your machine learning models to gain insights about your data.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "d1fee8d42f64342aeb29b519d85a1bc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4. What are the two types of regularization for linear regression, and what is the difference between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "89f6de5e321584d60f71306179e0b234",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "L1 or Lasso Regression adds a term to the cost function which reduces some smaller weights down to zero.\n",
    "\n",
    "L2 or Ridge Regression adds a term to the cost function which penalizes weights based on their size, bringing all of them closer to zero.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "1fc1e2260d2806a7d16c108f4f4d430a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5. Why is scaling input variables a necessary step before regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "0d7c9ea9ff2880bd79ebdd7863d077c6",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "Regularization adjusts feature weights depending on their magnitude.\n",
    "\n",
    "Feature weights themselves depend on both the feature importance and the magnitude of the input variable.\n",
    "\n",
    "Therefore, it's important to control for the magnitude of the input variable by scaling all features the same.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
